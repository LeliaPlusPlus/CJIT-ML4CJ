{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facf74aa-d764-420b-b975-84ed104951f4",
   "metadata": {},
   "source": [
    "# Logistic Regression Tutorial\n",
    "\n",
    "This notebook provides a brief tutorial on coding logistic regression in Python. We use the Climate and Economic Justice Screening Tool as our dataset. Please download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796ed7d-4089-4d95-8571-841bcd7d389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84091b49-284e-4bd5-b687-84cfe2143e22",
   "metadata": {},
   "source": [
    "## Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a076e1-382b-47a9-b0a1-947576a328cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/CEJST-v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05135622-30dc-4783-be90-7d6a50480b16",
   "metadata": {},
   "source": [
    "The islands, unfortunately, have missing data, so we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81049c30-5fd5-4e5d-bfc2-384b120aaebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "islands = ['American Samoa', 'Guam', 'Northern Mariana Islands', 'Puerto Rico', 'Virgin Islands']\n",
    "df = df[~df['State/Territory'].isin(islands)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a05d33-dc8a-4f48-8c73-83ca7b63251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1af9fa-6ac8-4990-ab9b-d5045cc6543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Percent Black or African American alone', 'Percent American Indian / Alaska Native', 'Percent Asian',\n",
    "            'Percent Native Hawaiian or Pacific', 'Percent White', 'Percent Hispanic or Latino', \n",
    "            'Percent age under 10', 'Percent age 10 to 64', 'Percent age over 64', 'Unemployment (percent)', \n",
    "            'Percent of individuals < 100% Federal Poverty Line', \n",
    "            'Percent of individuals below 200% Federal Poverty Line',\n",
    "            'Percent individuals age 25 or over with less than high school degree', 'Linguistic isolation (percent)', \n",
    "            'Housing burden (percent)', 'Percent pre-1960s housing (lead paint indicator)', \n",
    "            'Median value ($) of owner-occupied housing units', \n",
    "            \"Share of the tract's land area that is covered by impervious surface or cropland as a percent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62450a98-c491-489c-a2f7-a431c308c9de",
   "metadata": {},
   "source": [
    "The historic underinvestment indicator has missing values, but those places did not experience redlining, so we can reflect that with missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f6d85-2efc-4f3d-83a5-708b1f0cf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[features]\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae533577-cf98-43e8-b84d-0a5e2a2be813",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea3784-6996-446e-8385-496a04625b18",
   "metadata": {},
   "source": [
    "Let's perform hyperparameter tuning. Play around with the solvers and penalty parameter C. Make sure to use L2 penalty as a default parameter so that we can keep all of the features. Another useful default would be to increase the number of maximum iterations (e.g., 50000 or more); this means it will take longer to run, but that is okay. Use AUROC as your refit loss in order to account for our imbalanced labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7e6db-ef05-4b42-85c6-022960ee3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(X,y): \n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    search = ## YOUR CODE HERE\n",
    "    \n",
    "    print(\"Best parameter (CV AUROC score=%0.3f):\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    return search.best_estimator_, search.best_params_, pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13153496-0f74-4e05-9450-97d0bc9ce881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(X, y, params=None):\n",
    "    \n",
    "    # fit model\n",
    "    clf = LogisticRegression(**params, max_iter=500000, penalty='l2').fit(X, y) \n",
    "    \n",
    "    # get model coefficients \n",
    "    coef = clf.coef_.tolist()[0] + [clf.intercept_[0]]\n",
    "    \n",
    "    # Create coefficient and intercept table\n",
    "    summary_df = pd.DataFrame({'Coefficient': coef})\n",
    "    summary_df.index = list(data.columns) + ['Intercept']\n",
    "    summary_df['Odds Ratio'] = np.exp(summary_df['Coefficient'])\n",
    "    summary_df['Percentage Effect'] = 100 * (summary_df['Odds Ratio'] - 1)\n",
    "    \n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4656a-98e0-45cf-8403-40468c4257bc",
   "metadata": {},
   "source": [
    "We estimate the log odds of simultaneously being in the 90th percentile for property flood risk in 30 years and living in a low income census tract. Given the imbalanced nature of a labels with a 9:1 ratio, we have pretty decent performance as measured through AUROC, which is a great metric for imbalanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b17794-916f-46f6-b24b-58b039fd8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Greater than or equal to the 90th percentile for share of properties at risk of flood in 30 years and is low income?'\n",
    "X, y = data, df[label]\n",
    "estimator, params, results = find_best_model(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd0cbb-bcf8-4185-9ddd-bcdd585e4891",
   "metadata": {},
   "source": [
    "To address the line search warning, you can change the solver or add more iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b663d1-b3e4-4ccc-a39c-bc3210341ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_summary(X, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00b3d3-1bd8-4e87-a2df-dafff8f3a032",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "Coefficients for continuous, binary, and categorical features have different interpretations. \n",
    "\n",
    "Let's look at the continuous feature of the percent of white residents in a tract. The log odds are 0.92. The odds ratio is 2.51, meaning tracts with a higher percent of white residents have greater odds of being in the 90th percentile and low income than tracts with lower percent of white residents. In this case, census tracts with higher percentages of white residents are 151 percent **more** likely to be in the 90th percentile for expected building loss and low income.\n",
    "\n",
    "Let's look at a binary feature. Tracts that experienced historic underinvestment and remain low income are 21.1 percent **more** likely to be in the 90th percentile for share of properties at risk of flood in 30 years and low income compared to tracts that did not experience historic underinvestment.\n",
    "\n",
    "Recall that the intercept is the value we would get if we set the input features to 0. Sometimes it is interesting to interpret, and sometimes it is simply a statistical formality that might not have an interesting application to the domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
